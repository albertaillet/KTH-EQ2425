{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertaillet/KTH-EQ2425/blob/master/project3/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nsLb2bLCuIqY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        }
      ],
      "source": [
        "%pip install wandb --quiet # if using colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UkOe61L2uL9d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical, set_random_seed\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# for type hinting\n",
        "from typing import List\n",
        "from numpy import ndarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Yt0dIfJ92Ry-"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_VfouJcosXgM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ld4ydEuJ4TQq"
      },
      "outputs": [],
      "source": [
        "def create_config(\n",
        "    input_shape: tuple=(32, 32, 3),\n",
        "    num_classes: int=10,\n",
        "    activation: str=\"relu\",\n",
        "    output_activation: str='softmax',\n",
        "    conv_kernel_sizes: List[tuple]=[(5, 5), (3, 3), (3, 3)],\n",
        "    conv_strides: List[int]=[1, 1, 1],\n",
        "    conv_filters: List[int]=[24, 48, 96],\n",
        "    conv_activate: List[bool]=[True, True, False],\n",
        "    pool_kernel_sizes: List[tuple]=[(2, 2), (2, 2), (2, 2)],\n",
        "    pool_strides: List[int]=[2, 2, 2],\n",
        "    fully_connected_sizes: List[int]=[512],\n",
        "    dropout: bool=False,\n",
        "    dropout_rate: float=0.3,\n",
        "    batch_normalization: bool=False,\n",
        "    batch_size: int=64,\n",
        "    learning_rate: float=1e-3,\n",
        "    model_name: str=\"cifar10_model\",\n",
        "    data_shuffling: bool=False,\n",
        "    epochs: int=300,\n",
        "    optimizer: str=\"sgd\",\n",
        "    exponential_decay: bool=False,\n",
        "    exp_dec_learning_rate: float=0.1,\n",
        "    data_aug: bool=False,\n",
        "    monitor: str=\"val_loss\",\n",
        "    patience: int=15,\n",
        "    seed: int=1,\n",
        "    sweep: bool=False,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Creates a dictionary of hyperparameters for the model.\n",
        "    :param input_shape: shape of the input images\n",
        "    :param num_classes: number of classes in the dataset\n",
        "    :param activation: activation function to use on the convolutional layers and the fully connected layers\n",
        "    :param output_activation: activation function to use on the output layer\n",
        "    :param conv_kernel_sizes: list of kernel sizes for the convolutional layers\n",
        "    :param conv_strides: list of strides for the convolutional layers\n",
        "    :param conv_filters: list of number of filters for the convolutional layers\n",
        "    :param conv_activate: list of booleans indicating whether to use the activation function on the convolutional layers\n",
        "    :param pool_kernel_sizes: list of kernel sizes for the pooling layers\n",
        "    :param pool_strides: list of strides for the pooling layers\n",
        "    :param fully_connected_sizes: list of sizes for the fully connected layers\n",
        "    :param dropout: whether to use dropout\n",
        "    :param dropout_rate: dropout rate\n",
        "    :param batch_normalization: whether to use batch normalization\n",
        "    :param batch_size: batch size\n",
        "    :param learning_rate: learning rate\n",
        "    :param model_name: name of the model\n",
        "    :param data_shuffling: whether to shuffle the data\n",
        "    :param epochs: number of epochs\n",
        "    :param optimizer: optimizer to use\n",
        "    :param monitor: metrics to monitor\n",
        "    :param patience: patience for early stopping\n",
        "    :param seed: seed for the random number generators\n",
        "    :param sweep: whether to create a sweep config or a normal config\n",
        "    :return: config of hyperparameters or a sweep config\n",
        "    \"\"\"\n",
        "    if sweep:\n",
        "        del sweep\n",
        "        return {\n",
        "            k:(\n",
        "                v if isinstance(v, dict) else {'value': v}\n",
        "            ) \n",
        "            for k,v in locals().items()\n",
        "        }\n",
        "    else:\n",
        "        del sweep\n",
        "        return locals()\n",
        "\n",
        "def create_and_train_model(\n",
        "    input_shape: tuple,\n",
        "    num_classes: int,\n",
        "    activation: str,\n",
        "    output_activation: str,\n",
        "    conv_kernel_sizes: List[tuple],\n",
        "    conv_strides: List[int],\n",
        "    conv_filters: List[int],\n",
        "    conv_activate: List[bool],\n",
        "    pool_kernel_sizes: List[tuple],\n",
        "    pool_strides: List[int],\n",
        "    fully_connected_sizes: List[int],\n",
        "    dropout: bool,\n",
        "    dropout_rate: float,\n",
        "    batch_normalization: bool,\n",
        "    batch_size: int,\n",
        "    learning_rate: float,\n",
        "    model_name: str,\n",
        "    optimizer: str,\n",
        "    exponential_decay: bool,\n",
        "    data_aug: bool,\n",
        "    exp_dec_learning_rate: float,\n",
        "    seed: int,\n",
        "    **kwargs,\n",
        ") -> models.Model:\n",
        "    \"\"\"\n",
        "    Creates and trains a model on the CIFAR10 dataset.\n",
        "    :param input_shape: shape of the input images\n",
        "    :param num_classes: number of classes in the dataset\n",
        "    :param activation: activation function to use on the convolutional layers and the fully connected layers\n",
        "    :param output_activation: activation function to use on the output layer\n",
        "    :param conv_kernel_sizes: list of kernel sizes for the convolutional layers\n",
        "    :param conv_strides: list of strides for the convolutional layers\n",
        "    :param conv_filters: list of number of filters for the convolutional layers\n",
        "    :param conv_activate: list of booleans indicating whether to use the activation function on the convolutional layers\n",
        "    :param pool_kernel_sizes: list of kernel sizes for the pooling layers\n",
        "    :param pool_strides: list of strides for the pooling layers\n",
        "    :param fully_connected_sizes: list of sizes for the fully connected layers\n",
        "    :param dropout: whether to use dropout\n",
        "    :param dropout_rate: dropout rate\n",
        "    :param batch_normalization: whether to use batch normalization\n",
        "    :param batch_size: batch size\n",
        "    :param learning_rate: learning rate\n",
        "    :param model_name: name of the model\n",
        "    :param seed: seed for the random number generators\n",
        "    \"\"\"\n",
        "    # set random seed for reproducibility\n",
        "    set_seed(seed)\n",
        "    set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Create model\n",
        "    model = models.Sequential(name=model_name)\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "    # Normalize the pixel values to the range of [-0.5, 0.5].\n",
        "    model.add(layers.Lambda(lambda x: (x / 255.0) - 0.5, name=\"normalize\"))\n",
        "\n",
        "    names = [[f\"conv_{i}\", f\"pool_{i}\"] for i in range(1, len(conv_kernel_sizes)+1)]\n",
        "    # Convolutional layers.\n",
        "    for kernel_size, stride, filters, activate, pool_kernel_size, pool_stride, (conv_name, pool_name) in zip(\n",
        "        conv_kernel_sizes,\n",
        "        conv_strides,\n",
        "        conv_filters,\n",
        "        conv_activate,\n",
        "        pool_kernel_sizes,\n",
        "        pool_strides,\n",
        "        names,\n",
        "    ):\n",
        "        # Add convolutional layer\n",
        "        model.add(\n",
        "            layers.Conv2D(\n",
        "                kernel_size=kernel_size,\n",
        "                strides=stride,\n",
        "                padding=\"valid\",\n",
        "                filters=filters,\n",
        "                name=conv_name,\n",
        "            )\n",
        "        )\n",
        "        if activate:\n",
        "            # Add activation\n",
        "            if activation == \"leaky_relu\":\n",
        "                model.add(layers.LeakyReLU())\n",
        "            else:\n",
        "                model.add(layers.Activation(activation))\n",
        "\n",
        "            # Add batch normalization\n",
        "            if batch_normalization:\n",
        "                model.add(layers.BatchNormalization())\n",
        "        \n",
        "        # Add pooling\n",
        "        model.add(\n",
        "            layers.MaxPooling2D(\n",
        "                pool_size=pool_kernel_size, \n",
        "                strides=pool_stride, \n",
        "                name=pool_name,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    # Flatten the output of the convolutional layers.\n",
        "    model.add(layers.Flatten(name=\"flatten\"))\n",
        "\n",
        "    names = [f\"fc_{i}\" for i in range(1, len(fully_connected_sizes)+1)]\n",
        "    # Fully connected layers.\n",
        "    for size, name in zip(fully_connected_sizes, names):\n",
        "        model.add(layers.Dense(size, activation=activation, name=name))\n",
        "        if dropout:\n",
        "            model.add(layers.Dropout(dropout_rate))\n",
        "        if batch_normalization:\n",
        "            model.add(layers.BatchNormalization())\n",
        "    \n",
        "    # Output layer\n",
        "    model.add(layers.Dense(num_classes, activation=output_activation, name=f\"fc_{len(fully_connected_sizes)+1}\"))\n",
        "    \n",
        "    if exponential_decay:\n",
        "        learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=exp_dec_learning_rate,\n",
        "            decay_steps=10000,\n",
        "            decay_rate=0.9,\n",
        "        )   \n",
        "    \n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer_alg = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    elif optimizer == \"adam\":\n",
        "        optimizer_alg = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer_alg,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bRvZsqEO4TQs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  31/1563 [..............................] - ETA: 2:40 - loss: 2.2786 - accuracy: 0.1512"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [11], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m X_train_gen \u001b[39m=\u001b[39m datagen\u001b[39m.\u001b[39mflow(X_train, y_train, batch_size\u001b[39m=\u001b[39mfull_config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m X_test_gen \u001b[39m=\u001b[39m datagen\u001b[39m.\u001b[39mflow(X_test, y_test)\n\u001b[1;32m---> 32\u001b[0m model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m     33\u001b[0m     X_train,\n\u001b[0;32m     34\u001b[0m     y_train,\n\u001b[0;32m     35\u001b[0m     epochs\u001b[39m=\u001b[39mfull_config[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     36\u001b[0m     validation_data\u001b[39m=\u001b[39mX_test_gen,\n\u001b[0;32m     37\u001b[0m     \u001b[39m# callbacks=[],\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[39m# callbacks=[WandbCallback(generator=datagen), ES],\u001b[39;00m\n\u001b[0;32m     39\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[0;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[1;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[0;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[1;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[0;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[1;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\tf-env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "partial_config = {\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"epochs\": 1,\n",
        "    \"batch_size\": 64,\n",
        "    \"dropout\": False,\n",
        "    \"batch_normalization\": False,\n",
        "    \"data_shuffling\": False,\n",
        "    \"seed\": 1,\n",
        "    \"optimizer\": \"sgd\",\n",
        "    \"exponential_decay\": True,\n",
        "    \"data_aug\": True,\n",
        "    \"exp_dec_learning_rate\": 0.1,\n",
        "}\n",
        "full_config = create_config(**partial_config)\n",
        "model = create_and_train_model(**full_config)\n",
        "\n",
        "\n",
        "# TEST CODE #\n",
        "ES = tf.keras.callbacks.EarlyStopping(monitor=full_config['monitor'], patience=full_config['patience'])\n",
        "\n",
        "# create data generator\n",
        "datagen = ImageDataGenerator(\n",
        "                            rotation_range=30,\n",
        "                            horizontal_flip=True,\n",
        "                            vertical_flip=True,\n",
        "                            zoom_range=[0.5,1.0]\n",
        "                            )\n",
        "\n",
        "X_train_gen = datagen.flow(X_train, y_train, batch_size=full_config['batch_size'])\n",
        "X_test_gen = datagen.flow(X_test, y_test)\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=full_config['epochs'],\n",
        "    validation_data=X_test_gen,\n",
        "    # callbacks=[],\n",
        "    # callbacks=[WandbCallback(generator=datagen), ES],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8hgmHfR4TQp"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"project3\", \n",
        "    entity=\"eq2425_2022p3_aillet_bonato\",\n",
        "    config = full_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMfp_mnR5c0s"
      },
      "outputs": [],
      "source": [
        "ES = tf.keras.callbacks.EarlyStopping(monitor=full_config['monitor'], patience=full_config['patience'])\n",
        "if wandb.config['data_aug']:\n",
        "    # create data generator\n",
        "    datagen = ImageDataGenerator(\n",
        "                                rotation_range=30,\n",
        "                                horizontal_flip=True,\n",
        "                                vertical_flip=True,\n",
        "                                zoom_range=[0.5,1.0]\n",
        "                                )\n",
        "    X_train_gen = datagen.flow(X_train, y_train)\n",
        "    X_test_gen = datagen.flow(X_test, y_test)\n",
        "    \n",
        "    model.fit(\n",
        "        X_train_gen,\n",
        "        epochs=wandb.config['epochs'],\n",
        "        validation_data=X_test_gen,\n",
        "        shuffle=wandb.config['data_shuffling'],\n",
        "        callbacks=[WandbCallback(generator=datagen), ES],\n",
        "    )\n",
        "else:\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=wandb.config['epochs'],\n",
        "        batch_size=wandb.config['batch_size'],\n",
        "        validation_data=(X_test, y_test),\n",
        "        shuffle=wandb.config['data_shuffling'],\n",
        "        callbacks=[WandbCallback(), ES],\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjEw-UqNXnDl"
      },
      "source": [
        "## Sweeping "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRTxXFR0XnDo"
      },
      "outputs": [],
      "source": [
        "def train_function():\n",
        "    wandb.init()\n",
        "    ES = tf.keras.callbacks.EarlyStopping(monitor=wandb.config['monitor'], patience=wandb.config['patience'])    \n",
        "    \n",
        "    model = create_and_train_model(**wandb.config)\n",
        "    \n",
        "    if wandb.config['data_aug']:\n",
        "        # create data generator\n",
        "        datagen = ImageDataGenerator(\n",
        "                                    rotation_range=30,\n",
        "                                    horizontal_flip=True,\n",
        "                                    vertical_flip=True,\n",
        "                                    zoom_range=[0.5,1.0]\n",
        "                                    )\n",
        "        it = datagen.flow(X_train, y_train)\n",
        "\n",
        "        model.fit_generator(\n",
        "            it,\n",
        "            steps_per_epoch=len(X_train) / wandb.config['batch_size'],\n",
        "            epochs=wandb.config['epochs'],\n",
        "            batch_size=wandb.config['batch_size'],\n",
        "            validation_data=(X_test, y_test),\n",
        "            shuffle=wandb.config['data_shuffling'],\n",
        "            callbacks=[WandbCallback(), ES],\n",
        "        )\n",
        "    else:\n",
        "        model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            epochs=wandb.config['epochs'],\n",
        "            batch_size=wandb.config['batch_size'],\n",
        "            validation_data=(X_test, y_test),\n",
        "            shuffle=wandb.config['data_shuffling'],\n",
        "            callbacks=[WandbCallback(), ES],\n",
        "        )\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq-jC4f2XnDp"
      },
      "outputs": [],
      "source": [
        "# set up variables you don't want to sweep over, but that will be added to the sweep_config\n",
        "partial_config = {\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"epochs\": 300,\n",
        "    \"batch_size\": 64,\n",
        "    \"data_shuffling\": False,\n",
        "    \"seed\": 1,\n",
        "    \"optimizer\": \"sgd\",\n",
        "    \"exponential_decay\": True,\n",
        "    \"data_aug\": True,\n",
        "    \"exp_dec_learning_rate\": 0.1,\n",
        "    'conv_filters': [64, 128, 256],\n",
        "    'fully_connected_sizes': [512],\n",
        "    'conv_kernel_sizes': [(5, 5), (3, 3), (3, 3)],\n",
        "    'activation': {\n",
        "        'values': [\n",
        "            'relu',\n",
        "            'leaky_relu',\n",
        "        ]\n",
        "    },\n",
        "    'dropout' : {\n",
        "         'values': [\n",
        "             True,\n",
        "             False,\n",
        "         ]\n",
        "    },\n",
        "    'batch_normalization' : {\n",
        "        'values': [\n",
        "            True,\n",
        "            False,\n",
        "        ]\n",
        "    },\n",
        "}\n",
        "sweep_config = {\n",
        "    'name': 'sweep_4_C_D_E',\n",
        "    'method': 'grid',\n",
        "    'parameters': create_config(**partial_config, sweep=True)\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"project3\", entity=\"eq2425_2022p3_aillet_bonato\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLqlEUBsXnDq"
      },
      "outputs": [],
      "source": [
        "# start the sweep\n",
        "wandb.agent(sweep_id, function=train_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weZRtSMXXnDr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('tf-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "997f2eb5bf27194238f7c2d0fafc406765e99f44a8da62f11c5168f8901f17e1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
